{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import csv\n",
    "# import mysql.connector\n",
    "# import time\n",
    "   \n",
    "# class WebCrawler():\n",
    "# # Request Header\n",
    "#     headers=requests.utils.default_headers()\n",
    "#     headers.update(\n",
    "#         {\n",
    "#             'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'\n",
    "#         }\n",
    "#     )\n",
    "# # #  Constructor  \n",
    "# #     def __init__(self,path):\n",
    "# #         self.path=path\n",
    "        \n",
    "# # Storing Price with Url in database        \n",
    "#     def storPriceUrl(self,asin,url,price,channel):\n",
    "#         con=self.databaseCon()\n",
    "#         record = (asin, url,price)\n",
    "#         if(channel==1):\n",
    "#             try:\n",
    "#                 sql = \"INSERT INTO amz_prices (asin, url,price) VALUES (%s, %s,%s)\"\n",
    "#                 amzcursor=con.cursor()\n",
    "#                 amzcursor.execute(sql, record)\n",
    "#                 con.commit()\n",
    "                \n",
    "#             except mysql.connector.Error as err:\n",
    "#                 print(\"Something went wrong: {}\".format(err))\n",
    "                \n",
    "#             finally:\n",
    "#                 con.close()\n",
    "#         else:\n",
    "#             try:\n",
    "#                 sql = \"INSERT INTO searsprices (pid, url,price) VALUES (%s, %s,%s)\"\n",
    "#                 amzcursor=con.cursor()\n",
    "#                 amzcursor.execute(sql, record)\n",
    "#                 con.commit()\n",
    "                \n",
    "#             except mysql.connector.Error as err:\n",
    "#                 print(\"Something went wrong: {}\".format(err))\n",
    "                \n",
    "#             finally:\n",
    "#                 con.close()\n",
    "    \n",
    "        \n",
    "#  # For DataBase Connection    \n",
    "#     def databaseCon(self):\n",
    "#         con = mysql.connector.connect(\n",
    "#         host=\"localhost\",\n",
    "#         user=\"root\",\n",
    "#         passwd=\"\", \n",
    "#         database=\"sidea_old\"\n",
    "#         )\n",
    "        \n",
    "#         return con\n",
    "    \n",
    "    \n",
    "# # For Reading file    \n",
    "#     def fileRead(self,path):\n",
    "#         count=0\n",
    "#         with open(path) as csvfile:\n",
    "#             urlreader=csv.DictReader(csvfile,delimiter=',')\n",
    "#             for url in urlreader:       \n",
    "#                 response=requests.get(url['URL'],headers=WebCrawler.headers)\n",
    "#                 print(response)\n",
    "#                 data=BeautifulSoup(response.content,'lxml')\n",
    "#                 myData=data.find('div',attr={'id':'olpOfferListColumn'})\n",
    "#                 priceContainer=data.find_all('span',{'class':'a-size-large a-color-price olpOfferPrice a-text-bold'})\n",
    "#                 count=count+1\n",
    "#                 print(count)\n",
    "#                 if(len(priceContainer)>0):\n",
    "#                     price=priceContainer[0].text.strip()\n",
    "#                     self.storPriceUrl(url['SIN'],url['URL'],price,1)\n",
    "#                     time.sleep(4)\n",
    "        \n",
    "\n",
    "\n",
    "# crawler=WebCrawler()\n",
    "# crawler.fileRead('SinUrl.csv')\n",
    "\n",
    "# #     amzpricing=pd.DataFrame({'sin' : sinlist,\n",
    "# #                                 'url' : urllist,\n",
    "# #                                 'price' : pricelist})\n",
    "# # # Saving in csv\n",
    "# # # amzpricing.to_csv('AmazonPricing.csv')\n",
    "\n",
    "# # #Saving in database\n",
    "# # amzpricing.to_sql('amzprices', con = engine, if_exists = 'append', chunksize = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from csv import writer\n",
    "import mysql.connector\n",
    "import time\n",
    "\n",
    "\n",
    "class WebCrawler():\n",
    "# Request Header\n",
    "    headers=requests.utils.default_headers()\n",
    "    headers.update(\n",
    "        {\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36'\n",
    "        }\n",
    "    )\n",
    "\n",
    "# for crawling the whole website\n",
    "    def crawlingArea(self):\n",
    "        count=1\n",
    "        strcount=str(count)\n",
    "        while(count<=4):\n",
    "            url=\"https://www.zameen.com/Houses_Property/Karachi_Nazimabad-278-\"+strcount+\".html\"\n",
    "            parent_response=requests.get(url,headers=WebCrawler.headers)\n",
    "            print(parent_response)                \n",
    "            DOM=BeautifulSoup(parent_response.content,'lxml')\n",
    "            naz_house_links=DOM.find_all('a',{'class':'_7ac32433'},href=True)\n",
    "    # passing links \n",
    "            wholelist=self.crawlinghouseDisc(naz_house_links)\n",
    "            count+=1\n",
    "            strcount=str(count)\n",
    "            \n",
    "            \n",
    "# For Reading file    \n",
    "    def crawlinghouseDisc(self,links):\n",
    "        count=0\n",
    "        \n",
    "        for link in links:       \n",
    "            response=requests.get(\"https://www.zameen.com\"+link['href'],headers=WebCrawler.headers)\n",
    "            print(response)                \n",
    "            DOM=BeautifulSoup(response.content,'lxml')\n",
    "            main_container=DOM.findChild('main')\n",
    "            house_data=main_container.find_all('span',{'class':'_812aa185'})\n",
    "            house_type=house_data[0].text\n",
    "            price=house_data[1].findChild('div',{'class':'c4fc20ba'}).text\n",
    "            location=house_data[2].text\n",
    "            bath=house_data[3].text\n",
    "            size=house_data[4].text\n",
    "            beds=house_data[6].text\n",
    "            \n",
    "# appending item in the list            \n",
    "            data=[house_type,price,location,bath,size,beds]\n",
    "            self.fileWrite('NazimabadHouses.csv',data)\n",
    "            time.sleep(2)\n",
    "        \n",
    "        wholelist=[house_type,price,location,size,beds]\n",
    "        return wholelist\n",
    "\n",
    "# file write\n",
    "    def fileWrite(self,filepath,data):\n",
    "        with open(filepath,'a+', newline='') as csvobj:\n",
    "            csv_writer=writer(csvobj)\n",
    "            csv_writer.writerow(data)\n",
    "    \n",
    "    \n",
    "    \n",
    "crawler=WebCrawler()\n",
    "crawler.crawlingArea()\n",
    "\n",
    "#     amzpricing=pd.DataFrame({'sin' : sinlist,\n",
    "#                                 'url' : urllist,\n",
    "#                                 'price' : pricelist})\n",
    "# # Saving in csv\n",
    "# # amzpricing.to_csv('AmazonPricing.csv')\n",
    "\n",
    "# #Saving in database\n",
    "# amzpricing.to_sql('amzprices', con = engine, if_exists = 'append', chunksize = 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
